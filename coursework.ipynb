{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:27:00.826664: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-17 20:27:00.959264: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-17 20:27:01.036308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739824021.157376 1647672 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739824021.187177 1647672 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 20:27:01.388480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPool2D,\n",
    "    SpatialDropout2D,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Directory for Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Directory structure where all the datasets and compiled models will be stored\n",
    "* They will be stored in the directory called `data`\n",
    "* Compiled models will be stored in `data/models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = \"data\"\n",
    "DATASETS: dict[str, str] = {\n",
    "    \"mnist\": os.path.join(BASE_DATA_DIR, \"mnist\"),\n",
    "    \"emnist\": os.path.join(BASE_DATA_DIR, \"emnist\"),\n",
    "    \"handwritten_digits\": os.path.join(BASE_DATA_DIR, \"handwritten-digits-not-mnist\"),\n",
    "    \"usps\": os.path.join(BASE_DATA_DIR, \"usps\"),\n",
    "    \"models\": os.path.join(BASE_DATA_DIR, \"models\"),\n",
    "}\n",
    "\n",
    "for directory in DATASETS.values():\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions to Analyse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prints the number of samples for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_summary(dataset_name: str, count: int) -> None:\n",
    "    print(f\"{dataset_name:<30} : {count:>8,d} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Takes labels and a dataset name as input\n",
    "* It calculates and prints the distribution of classes within the provided labels\n",
    "* Specifically, it counts the occurrences of each unique label (representing different classes/digits) and then prints these counts along with the dataset name and the total number of samples\n",
    "* This is useful for understanding the balance or imbalance of classes in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(labels, dataset_name) -> None:\n",
    "    \"\"\"Print distribution of classes in a dataset\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{dataset_name} class distribution:\")\n",
    "    for digit, count in zip(unique, counts):\n",
    "        print(f\"Digit {digit}: {count} samples\")\n",
    "    print(f\"Total samples: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Displays the total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_total_samples(labels, dataset_name) -> None:\n",
    "    \"\"\"Print only total samples in a dataset\"\"\"\n",
    "    print(f\"{dataset_name}: {len(labels)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Displays the number 3\n",
    "* This is useful to visualise each dataset such that they can be normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit_3(images, labels, dataset_name: str) -> None:\n",
    "    \"\"\"Display first occurrence of digit 3 in dataset\"\"\"\n",
    "    idx = np.where(labels == 3)[0][0]\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(images[idx].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"Digit 3 from {dataset_name}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Built into Keres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    (x_train, labels_train), (x_test, labels_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    show_digit_3(x_train, labels_train, \"MNIST\")\n",
    "\n",
    "    return x_train, labels_train, x_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_mnist, labels_train_mnist, x_test, labels_test = load_mnist_data()\n",
    "x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)\n",
    "y_train_mnist = tf.keras.utils.to_categorical(labels_train_mnist, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Train                    :   60,000 samples\n",
      "MNIST Test                     :   10,000 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"MNIST Train\", len(labels_train_mnist))\n",
    "print_dataset_summary(\"MNIST Test\", len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Handwritten Digits Dataset (not in MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading dataset from directory\n",
    "* This dataset has transparent background with black foreground making it inconsistent from the other datasets so it is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_handwritten_digits():\n",
    "    images = []\n",
    "    labels = []\n",
    "    dataset_path: str = os.path.join(DATASETS[\"handwritten_digits\"], \"dataset\")\n",
    "\n",
    "    for label in range(10):\n",
    "        folder_path = os.path.join(dataset_path, str(label), str(label))\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Missing directory for label {label} - {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        file_count = len(\n",
    "            [name for name in os.listdir(folder_path) if name.endswith(\".png\")]\n",
    "        )\n",
    "        print(f\"Processing {file_count} samples for digit {label}\")\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".png\"):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with Image.open(img_path).convert(\"RGBA\") as rgba_img:\n",
    "                        bg = Image.new(\"RGBA\", rgba_img.size, (255, 255, 255, 255))\n",
    "                        bg.paste(rgba_img, (0, 0), rgba_img)\n",
    "                        img = bg.convert(\"L\")\n",
    "\n",
    "                    img = img.resize((28, 28))\n",
    "\n",
    "                    img_array = np.array(img).astype(\"float32\") / 255.0\n",
    "                    # invert so digits are white on black\n",
    "                    img_array = 1.0 - img_array\n",
    "                    images.append(img_array)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    show_digit_3(images, labels, \"Handwritten (not MNIST)\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10773 samples for digit 0\n",
      "Processing 10773 samples for digit 1\n",
      "Processing 10773 samples for digit 2\n",
      "Processing 10773 samples for digit 3\n",
      "Processing 10773 samples for digit 4\n",
      "Processing 10773 samples for digit 5\n",
      "Processing 10773 samples for digit 6\n",
      "Processing 10773 samples for digit 7\n",
      "Processing 10773 samples for digit 8\n",
      "Processing 10773 samples for digit 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "handwritten_images, handwritten_labels = load_handwritten_digits()\n",
    "handwritten_images = handwritten_images.reshape(-1, 28, 28, 1)\n",
    "handwritten_labels_cat = tf.keras.utils.to_categorical(handwritten_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handwritten Digits (not MNIST) :  107,730 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"Handwritten Digits (not MNIST)\", len(handwritten_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading EMNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emnist_data():\n",
    "    train_file = os.path.join(DATASETS[\"emnist\"], \"emnist-digits-train.csv\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(train_file):\n",
    "            print(f\"Error: EMNIST data file not found at {train_file}\")\n",
    "            print(\"\\nAvailable files in EMNIST directory:\")\n",
    "            for file in os.listdir(DATASETS[\"emnist\"]):\n",
    "                print(f\"- {file}\")\n",
    "            raise FileNotFoundError(f\"EMNIST data file not found at {train_file}\")\n",
    "\n",
    "        print(f\"Loading EMNIST data from: {train_file}\")\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        data = pd.read_csv(train_file)\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Loaded CSV file is empty\")\n",
    "\n",
    "        labels = data.iloc[:, 0].values\n",
    "        pixels = data.iloc[:, 1:].values\n",
    "\n",
    "        images = pixels.reshape(-1, 28, 28)\n",
    "        images = images.transpose(0, 2, 1)\n",
    "        images = np.flip(images, axis=1)\n",
    "\n",
    "        images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "        show_digit_3(images, labels, \"EMNIST\")\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading EMNIST data: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EMNIST data from: data/emnist/emnist-digits-train.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emnist_images, emnist_labels = load_emnist_data()\n",
    "emnist_images = emnist_images.reshape(-1, 28, 28, 1)\n",
    "emnist_labels_cat = tf.keras.utils.to_categorical(emnist_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST                         :  239,999 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"EMNIST\", len(emnist_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading USPS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_usps_data():\n",
    "    import h5py\n",
    "\n",
    "    usps_path: str = os.path.join(DATASETS[\"usps\"], \"usps.h5\")\n",
    "\n",
    "    if not os.path.exists(usps_path):\n",
    "        raise FileNotFoundError(f\"USPS dataset file not found at {usps_path}\")\n",
    "\n",
    "    with h5py.File(usps_path, \"r\") as hf:\n",
    "        train = hf.get(\"train\")\n",
    "        X_train = train.get(\"data\")[:]\n",
    "        y_train = train.get(\"target\")[:]\n",
    "\n",
    "    print(\"Resizing USPS images from 16x16 to 28x28...\")\n",
    "    resized_images = []\n",
    "    for img in X_train:\n",
    "        img = img.reshape(16, 16)\n",
    "        img_resized = resize(img, (28, 28), anti_aliasing=True)\n",
    "        resized_images.append(img_resized)\n",
    "\n",
    "    images = np.array(resized_images)\n",
    "    images = images.astype(\"float32\")\n",
    "    if images.max() > 1.0:\n",
    "        images /= 255.0\n",
    "\n",
    "    show_digit_3(images, y_train, \"USPS\")\n",
    "\n",
    "    return images, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing USPS images from 16x16 to 28x28...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "usps_images, usps_labels = load_usps_data()\n",
    "usps_images = usps_images.reshape(-1, 28, 28, 1)\n",
    "usps_labels_cat = tf.keras.utils.to_categorical(usps_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS                           :    7,291 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"USPS\", len(usps_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined = np.concatenate(\n",
    "    [x_train_mnist, handwritten_images, emnist_images, usps_images]\n",
    ")\n",
    "y_train_combined = np.concatenate(\n",
    "    [y_train_mnist, handwritten_labels_cat, emnist_labels_cat, usps_labels_cat]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined, y_train_combined = shuffle(\n",
    "    x_train_combined, y_train_combined, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_combined, y_train_combined, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total combined samples: 415020\n",
      "Training samples: 373518\n",
      "Validation samples: 41502\n",
      "Total Combined                 :  415,020 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal combined samples: {len(x_train_combined)}\")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print_dataset_summary(\"Total Combined\", len(x_train_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define composite transform pipeline with varying probabilities\n",
    "* Combine transforms into one Compose:\n",
    "  - 100% chance (p=1.0) for ElasticTransform\n",
    "  - 20% chance (p=0.2) for GaussNoise\n",
    "  - 15% chance (p=0.15) for small 4x4 dropout\n",
    "  - 10% chance (p=0.1) for brightness/contrast\n",
    "  - 5% chance (p=0.05) to invert image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1623730/2113161089.py:6: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10, 20), p=0.2),\n",
      "/tmp/ipykernel_1623730/2113161089.py:7: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n"
     ]
    }
   ],
   "source": [
    "augmentation_transform = A.Compose(\n",
    "    [\n",
    "        # Geometric Transforms\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "        # Noise/Pixel-Level Transforms\n",
    "        A.GaussNoise(var_limit=(10, 20), p=0.2),\n",
    "        A.CoarseDropout(max_holes=1, max_height=4, max_width=4, fill_value=0.0, p=0.15),\n",
    "        # Color/Intensity Transforms\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.1),\n",
    "        A.InvertImg(p=0.05),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    # Prepare image for augmentation\n",
    "    image_0_1 = x_train[i, :, :, 0]\n",
    "    image_255 = (image_0_1 * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply augmentation pipeline\n",
    "    augmented = augmentation_transform(image=image_255)\n",
    "    aug_img_255 = augmented[\"image\"]\n",
    "\n",
    "    # Post-process augmented image\n",
    "    aug_img_01 = aug_img_255.astype(np.float32) / 255.0\n",
    "    aug_img_01 = np.expand_dims(aug_img_01, axis=-1)\n",
    "\n",
    "    augmented_images.append(aug_img_01)\n",
    "    augmented_labels.append(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combine augmented dataset with original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "augmented_images = np.array(augmented_images, dtype=np.float32)\n",
    "augmented_labels = np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and augmented datasets\n",
    "original_size = len(x_train)\n",
    "x_train = np.concatenate([x_train, augmented_images], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 373518 | After augmentation: 747036\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original training size: {original_size} | After augmentation: {len(x_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras ImageDataGenerator for on-the-fly augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "steps_per_epoch = len(x_train) // 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After augmentation (per epoch):\n",
      "Training samples (now doubled): 747036\n",
      "Augmented samples per epoch: 747008\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAfter augmentation (per epoch):\")\n",
    "print(f\"Training samples (now doubled): {len(x_train)}\")\n",
    "print(f\"Augmented samples per epoch: {steps_per_epoch * 256}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting `clipnorm=1.0` to effectively \"snip\" gradients whose norm > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 13:59:20.394800: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "clipped_adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-3,  # default LR\n",
    "    clipnorm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 convolutional blocks with increasing filters (32 → 64)\n",
    "* Regularization techniques:\n",
    "* BatchNormalization after each conv layer\n",
    "* SpatialDropout2D (20%) after each conv block\n",
    "* L2 regularization in dense layer\n",
    "* Dropout (50%) before final classification\n",
    "* Final 10-way softmax for classification (likely MNIST digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-17 20:27:14.333186: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        # --- Block 1 ---\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D((2, 2)),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # --- Block 2 (Modified: 96 filters) ---\n",
    "        Conv2D(96, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(96, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D((2, 2)),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # --- Block 3 ---\n",
    "        Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D((2, 2)),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # --- Dense layers (Modified: 1050 units) ---\n",
    "        Flatten(),\n",
    "        Dense(1050, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,744</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,720</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1050</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">538,650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1050</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,200</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1050</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,510</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │        \u001b[38;5;34m27,744\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │        \u001b[38;5;34m83,040\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m110,720\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1050\u001b[0m)           │       \u001b[38;5;34m538,650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1050\u001b[0m)           │         \u001b[38;5;34m4,200\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1050\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m10,510\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">934,064</span> (3.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m934,064\u001b[0m (3.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">930,940</span> (3.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m930,940\u001b[0m (3.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,124</span> (12.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,124\u001b[0m (12.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Configures the training process for the neural network with three key components:\n",
    "\n",
    "* Optimizer:\n",
    "  * Uses a custom clipped_adam optimizer\n",
    "  * Likely an Adam optimizer with gradient clipping to prevent exploding gradients\n",
    "  * Gradient clipping helps stabilize training\n",
    "\n",
    "* Loss Function:\n",
    "  * Categorical_crossentropy: Standard loss function for multi-class classification\n",
    "  * Appropriate for the 10-class MNIST digit classification task\n",
    "  * Expects one-hot encoded labels\n",
    "\n",
    "* Metrics:\n",
    "  * Tracks accuracy during training and validation\n",
    "  * Will show percentage of correctly classified images\n",
    "  * This compilation step must be done before training the model \n",
    "\n",
    "* It defines how the model will:\n",
    "  * Update weights (optimizer)\n",
    "  * Calculate errors (loss)\n",
    "  * Measure performance (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=clipped_adam,  # using the clipped optimizer\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Early Stopping\n",
    "   * Prevents overfitting by stopping training when validation loss stops improving\n",
    "   * Waits 5 epochs (patience=5) before stopping\n",
    "   * Restores the best weights found during training\n",
    "2. Model Checkpoint\n",
    "   * Saves the model when validation accuracy improves\n",
    "   * Only keeps the best performing model\n",
    "   * Saves to 'best_model.h5' in the models directory\n",
    "3. Learning Rate Reduction\n",
    "   * Reduces learning rate when validation loss plateaus\n",
    "   * Reduces by 20% (factor=0.2) after 3 epochs without improvement\n",
    "   * Won't reduce below 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(DATASETS[\"models\"], \"best_model.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trains the model using augmented data via datagen.flow\n",
    "* Uses batch size of 256 images\n",
    "* Validates on non-augmented validation set\n",
    "* Runs for 5 epochs\n",
    "* Uses the callbacks for early stopping, checkpointing, and LR reduction\n",
    "* Stores training history in history variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 83/422\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:24\u001b[0m 602ms/step - accuracy: 0.5108 - loss: 5.4161"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=256),\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=150,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saves the final model state after training\n",
    "* Stores it as 'final_model.h5' in the models directory\n",
    "* Different from the checkpoint callback which saves the best model during training\n",
    "* Includes model architecture, weights, and training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(DATASETS[\"models\"], \"final_model.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
