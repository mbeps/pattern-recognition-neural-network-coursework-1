{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Training Set: 60000 samples\n",
      "MNIST Test Set: 10000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total MNIST samples: 60000\n",
      "\n",
      "After splitting:\n",
      "Training samples: 54000\n",
      "Validation samples: 6000\n",
      "\n",
      "MNIST Dataset:\n",
      "----------------------------------------\n",
      "MNIST Train                    :   60,000 samples\n",
      "MNIST Test                     :   10,000 samples\n",
      "\n",
      "After Train/Val Split:\n",
      "----------------------------------------\n",
      "Training Set                   :   54,000 samples\n",
      "Validation Set                 :    6,000 samples\n",
      "\n",
      "Applying Elastic, Gaussian Noise, CoarseDropout, + Minimal Color Variation ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_187129/2722452502.py:252: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10, 20), p=0.2),\n",
      "/tmp/ipykernel_187129/2722452502.py:253: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 54000 | After augmentation: 108000\n",
      "\n",
      "After augmentation (per epoch):\n",
      "Training samples (now doubled): 108000\n",
      "Augmented samples per epoch: 107776\n",
      "Epoch 1/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - accuracy: 0.7481 - loss: 3.1366"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 465ms/step - accuracy: 0.7483 - loss: 3.1331 - val_accuracy: 0.8995 - val_loss: 0.5429 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.9324 - loss: 0.3978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 464ms/step - accuracy: 0.9324 - loss: 0.3978 - val_accuracy: 0.9903 - val_loss: 0.1968 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 465ms/step - accuracy: 0.9462 - loss: 0.3259 - val_accuracy: 0.9887 - val_loss: 0.1882 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step - accuracy: 0.9532 - loss: 0.3077"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 463ms/step - accuracy: 0.9532 - loss: 0.3077 - val_accuracy: 0.9910 - val_loss: 0.1842 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 465ms/step - accuracy: 0.9561 - loss: 0.2897 - val_accuracy: 0.9882 - val_loss: 0.1867 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                        ReduceLROnPlateau)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Dense,\n",
    "                                     Dropout, Flatten, MaxPool2D,\n",
    "                                     SpatialDropout2D)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== NEW IMPORTS FOR ELASTIC TRANSFORM + GAUSSIAN NOISE + COARSEDROPOUT + COLOR TRANSFORMS ==========\n",
    "import albumentations as A\n",
    "import cv2\n",
    "# ========================================================\n",
    "\n",
    "# Create consistent directory structure\n",
    "BASE_DATA_DIR = 'data'\n",
    "DATASETS: dict[str, str] = {\n",
    "    'mnist': os.path.join(BASE_DATA_DIR, 'mnist'),\n",
    "    'emnist': os.path.join(BASE_DATA_DIR, 'emnist'),\n",
    "    'handwritten_digits': os.path.join(BASE_DATA_DIR, 'handwritten-digits-not-mnist'),\n",
    "    'usps': os.path.join(BASE_DATA_DIR, 'usps'),\n",
    "    'models': os.path.join(BASE_DATA_DIR, 'models')\n",
    "}\n",
    "\n",
    "for directory in DATASETS.values():\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "def print_dataset_summary(dataset_name, count):\n",
    "    print(f\"{dataset_name:<30} : {count:>8,d} samples\")\n",
    "    \n",
    "\n",
    "def print_class_distribution(labels, dataset_name):\n",
    "    \"\"\"Print distribution of classes in a dataset\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{dataset_name} class distribution:\")\n",
    "    for digit, count in zip(unique, counts):\n",
    "        print(f\"Digit {digit}: {count} samples\")\n",
    "    print(f\"Total samples: {len(labels)}\")\n",
    "\n",
    "def print_total_samples(labels, dataset_name):\n",
    "    \"\"\"Print only total samples in a dataset\"\"\"\n",
    "    print(f\"{dataset_name}: {len(labels)} samples\")\n",
    "\n",
    "def show_digit_3(images, labels, dataset_name):\n",
    "    \"\"\"Display first occurrence of digit 3 in dataset\"\"\"\n",
    "    idx = np.where(labels == 3)[0][0]\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(images[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Digit 3 from {dataset_name}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def load_mnist_data():\n",
    "    (x_train, labels_train), (x_test, labels_test) = mnist.load_data()\n",
    "    print_total_samples(labels_train, \"MNIST Training Set\")\n",
    "    print_total_samples(labels_test, \"MNIST Test Set\")\n",
    "    \n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "    show_digit_3(x_train, labels_train, \"MNIST\")\n",
    "    \n",
    "    return x_train, labels_train, x_test, labels_test\n",
    "\n",
    "def load_handwritten_digits():\n",
    "    images = []\n",
    "    labels = []\n",
    "    dataset_path = os.path.join(DATASETS['handwritten_digits'], 'dataset')\n",
    "    \n",
    "    print(f\"\\nProcessing Handwritten Digits Dataset (not in MNIST) from: {dataset_path}\")\n",
    "    \n",
    "    for label in range(10):\n",
    "        folder_path = os.path.join(dataset_path, str(label), str(label))\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Missing directory for label {label} - {folder_path}\")\n",
    "            continue\n",
    "            \n",
    "        file_count = len([name for name in os.listdir(folder_path) if name.endswith('.png')])\n",
    "        print(f\"Processing {file_count} samples for digit {label}\")\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.png'):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with Image.open(img_path).convert(\"RGBA\") as rgba_img:\n",
    "                        bg = Image.new(\"RGBA\", rgba_img.size, (255, 255, 255, 255))\n",
    "                        bg.paste(rgba_img, (0, 0), rgba_img)\n",
    "                        img = bg.convert('L')\n",
    "                    \n",
    "                    img = img.resize((28, 28))\n",
    "                    \n",
    "                    img_array = np.array(img).astype('float32') / 255.0\n",
    "                    # invert so digits are white on black\n",
    "                    img_array = 1.0 - img_array  \n",
    "                    images.append(img_array)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    show_digit_3(images, labels, \"Handwritten (not MNIST)\")\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def load_emnist_data():\n",
    "    train_file = os.path.join(DATASETS['emnist'], 'emnist-digits-train.csv')\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(train_file):\n",
    "            print(f\"Error: EMNIST data file not found at {train_file}\")\n",
    "            print(\"\\nAvailable files in EMNIST directory:\")\n",
    "            for file in os.listdir(DATASETS['emnist']):\n",
    "                print(f\"- {file}\")\n",
    "            raise FileNotFoundError(f\"EMNIST data file not found at {train_file}\")\n",
    "        \n",
    "        print(f\"Loading EMNIST data from: {train_file}\")\n",
    "        \n",
    "        import pandas as pd\n",
    "        data = pd.read_csv(train_file)\n",
    "        \n",
    "        if data.empty:\n",
    "            raise ValueError(\"Loaded CSV file is empty\")\n",
    "        \n",
    "        labels = data.iloc[:, 0].values\n",
    "        pixels = data.iloc[:, 1:].values\n",
    "        \n",
    "        images = pixels.reshape(-1, 28, 28)\n",
    "        images = images.transpose(0, 2, 1)\n",
    "        images = np.flip(images, axis=1)\n",
    "        \n",
    "        images = images.astype('float32') / 255.0\n",
    "        \n",
    "        show_digit_3(images, labels, \"EMNIST\")\n",
    "        \n",
    "        return images, labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading EMNIST data: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def load_usps_data():\n",
    "    import h5py\n",
    "    usps_path = os.path.join(DATASETS['usps'], 'usps.h5')\n",
    "    \n",
    "    if not os.path.exists(usps_path):\n",
    "        raise FileNotFoundError(f\"USPS dataset file not found at {usps_path}\")\n",
    "        \n",
    "    print(f\"Loading USPS data from {usps_path}\")\n",
    "    \n",
    "    with h5py.File(usps_path, 'r') as hf:\n",
    "        train = hf.get('train')\n",
    "        X_train = train.get('data')[:]\n",
    "        y_train = train.get('target')[:]\n",
    "    \n",
    "    from skimage.transform import resize\n",
    "    print(\"Resizing USPS images from 16x16 to 28x28...\")\n",
    "    resized_images = []\n",
    "    for img in X_train:\n",
    "        img = img.reshape(16, 16)\n",
    "        img_resized = resize(img, (28, 28), anti_aliasing=True)\n",
    "        resized_images.append(img_resized)\n",
    "    \n",
    "    images = np.array(resized_images)\n",
    "    images = images.astype('float32')\n",
    "    if images.max() > 1.0:\n",
    "        images /= 255.0\n",
    "    \n",
    "    show_digit_3(images, y_train, \"USPS\")\n",
    "    \n",
    "    return images, y_train\n",
    "\n",
    "def prepare_data():\n",
    "    x_train_mnist, labels_train_mnist, x_test, labels_test = load_mnist_data()\n",
    "    x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)\n",
    "    y_train_mnist = tf.keras.utils.to_categorical(labels_train_mnist, 10)\n",
    "    \n",
    "    handwritten_images, handwritten_labels = load_handwritten_digits()\n",
    "    handwritten_images = handwritten_images.reshape(-1, 28, 28, 1)\n",
    "    handwritten_labels_cat = tf.keras.utils.to_categorical(handwritten_labels, 10)\n",
    "    \n",
    "    emnist_images, emnist_labels = load_emnist_data()\n",
    "    emnist_images = emnist_images.reshape(-1, 28, 28, 1)\n",
    "    emnist_labels_cat = tf.keras.utils.to_categorical(emnist_labels, 10)\n",
    "    \n",
    "    usps_images, usps_labels = load_usps_data()\n",
    "    usps_images = usps_images.reshape(-1, 28, 28, 1)\n",
    "    usps_labels_cat = tf.keras.utils.to_categorical(usps_labels, 10)\n",
    "    \n",
    "    x_train_combined = np.concatenate([\n",
    "        x_train_mnist, \n",
    "        handwritten_images, \n",
    "        emnist_images, \n",
    "        usps_images\n",
    "    ])\n",
    "    y_train_combined = np.concatenate([\n",
    "        y_train_mnist, \n",
    "        handwritten_labels_cat, \n",
    "        emnist_labels_cat, \n",
    "        usps_labels_cat\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\nTotal combined samples: {len(x_train_combined)}\")\n",
    "    \n",
    "    x_train_combined, y_train_combined = shuffle(\n",
    "        x_train_combined, \n",
    "        y_train_combined, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train_combined, \n",
    "        y_train_combined, \n",
    "        test_size=0.1, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAfter splitting:\")\n",
    "    print(f\"Training samples: {len(x_train)}\")\n",
    "    print(f\"Validation samples: {len(x_val)}\")\n",
    "    \n",
    "    print(\"\\nOriginal Datasets:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"MNIST Train\", len(labels_train_mnist))\n",
    "    print_dataset_summary(\"MNIST Test\", len(labels_test))\n",
    "    print_dataset_summary(\"Handwritten Digits (not MNIST)\", len(handwritten_labels))\n",
    "    print_dataset_summary(\"EMNIST\", len(emnist_labels))\n",
    "    print_dataset_summary(\"USPS\", len(usps_labels))\n",
    "    \n",
    "    total_samples = len(x_train_combined)\n",
    "    print(\"\\nCombined Dataset:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"Total Combined\", total_samples)\n",
    "    \n",
    "    print(\"\\nAfter Train/Val Split:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"Training Set\", len(x_train))\n",
    "    print_dataset_summary(\"Validation Set\", len(x_val))\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "# Prepare data\n",
    "x_train, y_train, x_val, y_val = prepare_data()\n",
    "\n",
    "# ========== OFFLINE AUGMENTATION BLOCK (ELASTIC + GAUSS NOISE + COARSEDROPOUT + COLOR SHIFTS) ==========\n",
    "print(\"\\nApplying Elastic, Gaussian Noise, CoarseDropout, + Minimal Color Variation ...\")\n",
    "\n",
    "# Combine transforms into one Compose:\n",
    "# - 100% chance (p=1.0) for ElasticTransform\n",
    "# - 20% chance (p=0.2) for GaussNoise\n",
    "# - 15% chance (p=0.15) for small 4x4 dropout\n",
    "# - 10% chance (p=0.1) for brightness/contrast\n",
    "# - 5% chance (p=0.05) to invert image\n",
    "augmentation_transform = A.Compose([\n",
    "    A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "    A.GaussNoise(var_limit=(10, 20), p=0.2),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=1,\n",
    "        max_height=4,\n",
    "        max_width=4,\n",
    "        fill_value=0.0,\n",
    "        p=0.15\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.2,  # ±20% brightness\n",
    "        contrast_limit=0.2,\n",
    "        p=0.1\n",
    "    ),\n",
    "    A.InvertImg(p=0.05)\n",
    "])\n",
    "\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    image_0_1 = x_train[i, :, :, 0]\n",
    "    image_255 = (image_0_1 * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply Albumentations pipeline\n",
    "    augmented = augmentation_transform(image=image_255)\n",
    "    aug_img_255 = augmented[\"image\"]\n",
    "\n",
    "    # Convert back to float [0,1], expand dims\n",
    "    aug_img_01 = aug_img_255.astype(np.float32) / 255.0\n",
    "    aug_img_01 = np.expand_dims(aug_img_01, axis=-1)\n",
    "\n",
    "    augmented_images.append(aug_img_01)\n",
    "    augmented_labels.append(y_train[i])\n",
    "\n",
    "augmented_images = np.array(augmented_images, dtype=np.float32)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Concatenate original + augmented\n",
    "original_size = len(x_train)\n",
    "x_train = np.concatenate([x_train, augmented_images], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_labels], axis=0)\n",
    "\n",
    "print(f\"Original training size: {original_size} | After augmentation: {len(x_train)}\")\n",
    "# ============================================================\n",
    "\n",
    "# Keras ImageDataGenerator for on-the-fly augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "steps_per_epoch = len(x_train) // 256\n",
    "print(f\"\\nAfter augmentation (per epoch):\")\n",
    "print(f\"Training samples (now doubled): {len(x_train)}\")\n",
    "print(f\"Augmented samples per epoch: {steps_per_epoch * 256}\")\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(DATASETS['models'], 'best_model.h5'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=256),\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=5,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "model.save(os.path.join(DATASETS['models'], 'final_model.h5'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
