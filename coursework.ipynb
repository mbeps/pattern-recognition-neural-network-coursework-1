{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Training Set: 60000 samples\n",
      "MNIST Test Set: 10000 samples\n",
      "Dataset URL: https://www.kaggle.com/datasets/jcprogjava/handwritten-digits-dataset-not-in-mnist\n",
      "\n",
      "Processing Handwritten Digits Dataset (not in MNIST) from: data/handwritten-digits-not-mnist/dataset\n",
      "Processing 10773 samples for digit 0\n",
      "Processing 10773 samples for digit 1\n",
      "Processing 10773 samples for digit 2\n",
      "Processing 10773 samples for digit 3\n",
      "Processing 10773 samples for digit 4\n",
      "Processing 10773 samples for digit 5\n",
      "Processing 10773 samples for digit 6\n",
      "Processing 10773 samples for digit 7\n",
      "Processing 10773 samples for digit 8\n",
      "Processing 10773 samples for digit 9\n",
      "\n",
      "Handwritten Digits Dataset (not in MNIST) Summary:\n",
      "----------------------------------------\n",
      "\n",
      "Handwritten Digits Dataset (not in MNIST) class distribution:\n",
      "Digit 0: 10773 samples\n",
      "Digit 1: 10773 samples\n",
      "Digit 2: 10773 samples\n",
      "Digit 3: 10773 samples\n",
      "Digit 4: 10773 samples\n",
      "Digit 5: 10773 samples\n",
      "Digit 6: 10773 samples\n",
      "Digit 7: 10773 samples\n",
      "Digit 8: 10773 samples\n",
      "Digit 9: 10773 samples\n",
      "Total samples: 107730\n",
      "Loading EMNIST data from: data/emnist/emnist-digits-train.csv\n",
      "Loaded data shape: (239999, 785)\n",
      "Labels shape: (239999,)\n",
      "Pixels shape: (239999, 784)\n",
      "EMNIST Dataset: 239999 samples\n",
      "Final images shape: (239999, 28, 28)\n",
      "Loading USPS data from data/usps/usps.h5\n",
      "Resizing USPS images from 16x16 to 28x28...\n",
      "USPS Dataset: 7291 samples\n",
      "\n",
      "Total combined samples: 415020\n",
      "\n",
      "After splitting:\n",
      "Training samples: 373518\n",
      "Validation samples: 41502\n",
      "\n",
      "Original Datasets:\n",
      "----------------------------------------\n",
      "MNIST Train                    :   60,000 samples\n",
      "MNIST Test                     :   10,000 samples\n",
      "Handwritten Digits (not MNIST) :  107,730 samples\n",
      "EMNIST                         :  239,999 samples\n",
      "USPS                           :    7,291 samples\n",
      "\n",
      "Combined Dataset:\n",
      "----------------------------------------\n",
      "Total Combined                 :  415,020 samples\n",
      "\n",
      "After Train/Val Split:\n",
      "----------------------------------------\n",
      "Training Set                   :  373,518 samples\n",
      "Validation Set                 :   41,502 samples\n",
      "\n",
      "After augmentation (per epoch):\n",
      "Original training samples: 373518\n",
      "Augmented samples per epoch: 373504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501ms/step - accuracy: 0.6492 - loss: 2.2125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1460/1460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m756s\u001b[0m 513ms/step - accuracy: 0.6493 - loss: 2.2119 - val_accuracy: 0.7557 - val_loss: 0.7698 - learning_rate: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, MaxPool2D, SpatialDropout2D,\n",
    "    Flatten, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Create consistent directory structure\n",
    "BASE_DATA_DIR = 'data'\n",
    "DATASETS: dict[str, str] = {\n",
    "    'mnist': os.path.join(BASE_DATA_DIR, 'mnist'),\n",
    "    'emnist': os.path.join(BASE_DATA_DIR, 'emnist'),\n",
    "    'handwritten_digits': os.path.join(BASE_DATA_DIR, 'handwritten-digits-not-mnist'),\n",
    "    'usps': os.path.join(BASE_DATA_DIR, 'usps'),\n",
    "    'models': os.path.join(BASE_DATA_DIR, 'models')\n",
    "}\n",
    "\n",
    "# Create all necessary directories\n",
    "for directory in DATASETS.values():\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_dataset_summary(dataset_name, count):\n",
    "    print(f\"{dataset_name:<30} : {count:>8,d} samples\")\n",
    "    \n",
    "\n",
    "def print_class_distribution(labels, dataset_name):\n",
    "    \"\"\"Print distribution of classes in a dataset\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{dataset_name} class distribution:\")\n",
    "    for digit, count in zip(unique, counts):\n",
    "        print(f\"Digit {digit}: {count} samples\")\n",
    "    print(f\"Total samples: {len(labels)}\")\n",
    "\n",
    "\n",
    "def print_total_samples(labels, dataset_name):\n",
    "    \"\"\"Print only total samples in a dataset\"\"\"\n",
    "    print(f\"{dataset_name}: {len(labels)} samples\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_handwritten_digits():\n",
    "    \"\"\"Load and process Handwritten Digits Dataset (not in MNIST).\"\"\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    api.dataset_download_files(\n",
    "        'jcprogjava/handwritten-digits-dataset-not-in-mnist',\n",
    "        path=DATASETS['handwritten_digits'],\n",
    "        unzip=True,\n",
    "        force=True\n",
    "    )\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    dataset_path = os.path.join(DATASETS['handwritten_digits'], 'dataset')\n",
    "    \n",
    "    print(f\"\\nProcessing Handwritten Digits Dataset (not in MNIST) from: {dataset_path}\")\n",
    "    \n",
    "    for label in range(10):\n",
    "        # Single folder_path assignment with correct path structure\n",
    "        folder_path = os.path.join(dataset_path, str(label), str(label))  # Path to digit/digit folder\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Missing directory for label {label} - {folder_path}\")\n",
    "            continue\n",
    "            \n",
    "        file_count = len([name for name in os.listdir(folder_path) if name.endswith('.png')])\n",
    "        print(f\"Processing {file_count} samples for digit {label}\")\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.png'):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('L')\n",
    "                    img = img.resize((28, 28))\n",
    "                    img_array = np.array(img)\n",
    "                    img_array = img_array.astype('float32') / 255.0\n",
    "                    img_array = 1.0 - img_array  # Invert to white-on-black\n",
    "                    images.append(img_array)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(\"\\nHandwritten Digits Dataset (not in MNIST) Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_class_distribution(labels, \"Handwritten Digits Dataset (not in MNIST)\")\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_emnist_data():\n",
    "    \"\"\"Load and process EMNIST digits dataset from local directory.\"\"\"\n",
    "    # Construct file paths using the DATASETS dictionary\n",
    "    train_file = os.path.join(DATASETS['emnist'], 'emnist-digits-train.csv')\n",
    "    \n",
    "    try:\n",
    "        # First check if file exists\n",
    "        if not os.path.exists(train_file):\n",
    "            print(f\"Error: EMNIST data file not found at {train_file}\")\n",
    "            print(\"\\nAvailable files in EMNIST directory:\")\n",
    "            for file in os.listdir(DATASETS['emnist']):\n",
    "                print(f\"- {file}\")\n",
    "            raise FileNotFoundError(f\"EMNIST data file not found at {train_file}\")\n",
    "        \n",
    "        print(f\"Loading EMNIST data from: {train_file}\")\n",
    "        \n",
    "        # Load data using pandas\n",
    "        import pandas as pd  # Adding import here in case it was missing\n",
    "        data = pd.read_csv(train_file)\n",
    "        \n",
    "        if data.empty:\n",
    "            raise ValueError(\"Loaded CSV file is empty\")\n",
    "            \n",
    "        print(f\"Loaded data shape: {data.shape}\")\n",
    "        \n",
    "        # Extract labels and pixels\n",
    "        labels = data.iloc[:, 0].values\n",
    "        pixels = data.iloc[:, 1:].values\n",
    "        \n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Pixels shape: {pixels.shape}\")\n",
    "        \n",
    "        # Reshape and reorient images\n",
    "        images = pixels.reshape(-1, 28, 28)\n",
    "        images = images.transpose(0, 2, 1)  # Correct orientation\n",
    "        images = np.flip(images, axis=1)  # Vertical flip\n",
    "        \n",
    "        # Normalize pixel values to [0, 1]\n",
    "        images = images.astype('float32') / 255.0\n",
    "        \n",
    "        print_total_samples(labels, \"EMNIST Dataset\")\n",
    "        print(f\"Final images shape: {images.shape}\")\n",
    "        \n",
    "        # Verify no NaN values\n",
    "        if np.isnan(images).any():\n",
    "            raise ValueError(\"NaN values found in processed images\")\n",
    "            \n",
    "        return images, labels\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(str(e))\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading EMNIST data: {str(e)}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def load_mnist_data():\n",
    "    \"\"\"Load and process MNIST dataset.\"\"\"\n",
    "    (x_train, labels_train), (x_test, labels_test) = mnist.load_data()\n",
    "    print_total_samples(labels_train, \"MNIST Training Set\")\n",
    "    print_total_samples(labels_test, \"MNIST Test Set\")\n",
    "    \n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    return x_train, labels_train, x_test, labels_test\n",
    "\n",
    "\n",
    "def load_usps_data():\n",
    "    \"\"\"Load and process USPS dataset from local file.\"\"\"\n",
    "    import h5py\n",
    "    usps_path = os.path.join(DATASETS['usps'], 'usps.h5')\n",
    "    \n",
    "    if not os.path.exists(usps_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"USPS dataset file not found at {usps_path}. \"\n",
    "            \"Please download usps.h5 from Kaggle and place it in the {DATASETS['usps']} directory\"\n",
    "        )\n",
    "        \n",
    "    print(f\"Loading USPS data from {usps_path}\")\n",
    "    \n",
    "    with h5py.File(usps_path, 'r') as hf:\n",
    "        train = hf.get('train')\n",
    "        X_train = train.get('data')[:]\n",
    "        y_train = train.get('target')[:]\n",
    "    \n",
    "    # Reshape and resize images from 16x16 to 28x28\n",
    "    from skimage.transform import resize\n",
    "    print(\"Resizing USPS images from 16x16 to 28x28...\")\n",
    "    resized_images = []\n",
    "    for img in X_train:\n",
    "        img = img.reshape(16, 16)\n",
    "        img_resized = resize(img, (28, 28), anti_aliasing=True)\n",
    "        resized_images.append(img_resized)\n",
    "    \n",
    "    images = np.array(resized_images)\n",
    "    \n",
    "    # Ensure values are in [0, 1] range\n",
    "    images = images.astype('float32')\n",
    "    if images.max() > 1.0:\n",
    "        images /= 255.0\n",
    "    \n",
    "    print_total_samples(y_train, \"USPS Dataset\")\n",
    "    return images, y_train\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Prepare and combine all datasets.\"\"\"\n",
    "    # Load MNIST\n",
    "    x_train_mnist, labels_train_mnist, x_test, labels_test = load_mnist_data()\n",
    "    x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)\n",
    "    y_train_mnist = tf.keras.utils.to_categorical(labels_train_mnist, 10)\n",
    "    \n",
    "    # Load handwritten digits data\n",
    "    handwritten_images, handwritten_labels = load_handwritten_digits()\n",
    "    handwritten_images = handwritten_images.reshape(-1, 28, 28, 1)\n",
    "    handwritten_labels_cat = tf.keras.utils.to_categorical(handwritten_labels, 10)\n",
    "    \n",
    "    # Load EMNIST\n",
    "    emnist_images, emnist_labels = load_emnist_data()\n",
    "    emnist_images = emnist_images.reshape(-1, 28, 28, 1)\n",
    "    emnist_labels_cat = tf.keras.utils.to_categorical(emnist_labels, 10)\n",
    "    \n",
    "    # Load USPS\n",
    "    usps_images, usps_labels = load_usps_data()\n",
    "    usps_images = usps_images.reshape(-1, 28, 28, 1)\n",
    "    usps_labels_cat = tf.keras.utils.to_categorical(usps_labels, 10)\n",
    "    \n",
    "    # Combine datasets\n",
    "    x_train_combined = np.concatenate([x_train_mnist, handwritten_images, emnist_images, usps_images])\n",
    "    y_train_combined = np.concatenate([y_train_mnist, handwritten_labels_cat, emnist_labels_cat, usps_labels_cat])\n",
    "    \n",
    "    print(f\"\\nTotal combined samples: {len(x_train_combined)}\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    x_train_combined, y_train_combined = shuffle(x_train_combined, y_train_combined, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train_combined, y_train_combined, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAfter splitting:\")\n",
    "    print(f\"Training samples: {len(x_train)}\")\n",
    "    print(f\"Validation samples: {len(x_val)}\")\n",
    "    \n",
    "    print(\"\\nOriginal Datasets:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"MNIST Train\", len(labels_train_mnist))\n",
    "    print_dataset_summary(\"MNIST Test\", len(labels_test))\n",
    "    print_dataset_summary(\"Handwritten Digits (not MNIST)\", len(handwritten_labels))\n",
    "    print_dataset_summary(\"EMNIST\", len(emnist_labels))\n",
    "    print_dataset_summary(\"USPS\", len(usps_labels))\n",
    "    \n",
    "    # After combining\n",
    "    total_samples = len(x_train_combined)\n",
    "    print(\"\\nCombined Dataset:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"Total Combined\", total_samples)\n",
    "    \n",
    "    # After splitting\n",
    "    print(\"\\nAfter Train/Val Split:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"Training Set\", len(x_train))\n",
    "    print_dataset_summary(\"Validation Set\", len(x_val))\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "x_train, y_train, x_val, y_val = prepare_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "steps_per_epoch = len(x_train) // 256  # batch_size = 256\n",
    "print(f\"\\nAfter augmentation (per epoch):\")\n",
    "print(f\"Original training samples: {len(x_train)}\")\n",
    "print(f\"Augmented samples per epoch: {steps_per_epoch * 256}\")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(DATASETS['models'], 'best_model.h5'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=256),\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save final model in HDF5 format\n",
    "model.save(os.path.join(DATASETS['models'], 'final_model.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
