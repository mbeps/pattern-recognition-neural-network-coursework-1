{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 20:27:00.826664: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-17 20:27:00.959264: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-17 20:27:01.036308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739824021.157376 1647672 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739824021.187177 1647672 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 20:27:01.388480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPool2D,\n",
    "    SpatialDropout2D,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Directory for Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Directory structure where all the datasets and compiled models will be stored\n",
    "* They will be stored in the directory called `data`\n",
    "* Compiled models will be stored in `data/models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_DIR = \"data\"\n",
    "DATASETS: dict[str, str] = {\n",
    "    \"mnist\": os.path.join(BASE_DATA_DIR, \"mnist\"),\n",
    "    \"emnist\": os.path.join(BASE_DATA_DIR, \"emnist\"),\n",
    "    \"handwritten_digits\": os.path.join(BASE_DATA_DIR, \"handwritten-digits-not-mnist\"),\n",
    "    \"usps\": os.path.join(BASE_DATA_DIR, \"usps\"),\n",
    "    \"models\": os.path.join(BASE_DATA_DIR, \"models\"),\n",
    "}\n",
    "\n",
    "for directory in DATASETS.values():\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions to Analyse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prints the number of samples for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_summary(dataset_name: str, count: int) -> None:\n",
    "    print(f\"{dataset_name:<30} : {count:>8,d} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Takes labels and a dataset name as input\n",
    "* It calculates and prints the distribution of classes within the provided labels\n",
    "* Specifically, it counts the occurrences of each unique label (representing different classes/digits) and then prints these counts along with the dataset name and the total number of samples\n",
    "* This is useful for understanding the balance or imbalance of classes in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(labels, dataset_name) -> None:\n",
    "    \"\"\"Print distribution of classes in a dataset\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{dataset_name} class distribution:\")\n",
    "    for digit, count in zip(unique, counts):\n",
    "        print(f\"Digit {digit}: {count} samples\")\n",
    "    print(f\"Total samples: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Displays the total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_total_samples(labels, dataset_name) -> None:\n",
    "    \"\"\"Print only total samples in a dataset\"\"\"\n",
    "    print(f\"{dataset_name}: {len(labels)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Displays the number 3\n",
    "* This is useful to visualise each dataset such that they can be normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit_3(images, labels, dataset_name: str) -> None:\n",
    "    \"\"\"Display first occurrence of digit 3 in dataset\"\"\"\n",
    "    idx = np.where(labels == 3)[0][0]\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(images[idx].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"Digit 3 from {dataset_name}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Built into Keres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    (x_train, labels_train), (x_test, labels_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    show_digit_3(x_train, labels_train, \"MNIST\")\n",
    "\n",
    "    return x_train, labels_train, x_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_mnist, labels_train_mnist, x_test, labels_test = load_mnist_data()\n",
    "x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)\n",
    "y_train_mnist = tf.keras.utils.to_categorical(labels_train_mnist, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Train                    :   60,000 samples\n",
      "MNIST Test                     :   10,000 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"MNIST Train\", len(labels_train_mnist))\n",
    "print_dataset_summary(\"MNIST Test\", len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Handwritten Digits Dataset (not in MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading dataset from directory\n",
    "* This dataset has transparent background with black foreground making it inconsistent from the other datasets so it is processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_handwritten_digits():\n",
    "    images = []\n",
    "    labels = []\n",
    "    dataset_path: str = os.path.join(DATASETS[\"handwritten_digits\"], \"dataset\")\n",
    "\n",
    "    for label in range(10):\n",
    "        folder_path = os.path.join(dataset_path, str(label), str(label))\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Missing directory for label {label} - {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        file_count = len(\n",
    "            [name for name in os.listdir(folder_path) if name.endswith(\".png\")]\n",
    "        )\n",
    "        print(f\"Processing {file_count} samples for digit {label}\")\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".png\"):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    with Image.open(img_path).convert(\"RGBA\") as rgba_img:\n",
    "                        bg = Image.new(\"RGBA\", rgba_img.size, (255, 255, 255, 255))\n",
    "                        bg.paste(rgba_img, (0, 0), rgba_img)\n",
    "                        img = bg.convert(\"L\")\n",
    "\n",
    "                    img = img.resize((28, 28))\n",
    "\n",
    "                    img_array = np.array(img).astype(\"float32\") / 255.0\n",
    "                    # invert so digits are white on black\n",
    "                    img_array = 1.0 - img_array\n",
    "                    images.append(img_array)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    show_digit_3(images, labels, \"Handwritten (not MNIST)\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10773 samples for digit 0\n",
      "Processing 10773 samples for digit 1\n",
      "Processing 10773 samples for digit 2\n",
      "Processing 10773 samples for digit 3\n",
      "Processing 10773 samples for digit 4\n",
      "Processing 10773 samples for digit 5\n",
      "Processing 10773 samples for digit 6\n",
      "Processing 10773 samples for digit 7\n",
      "Processing 10773 samples for digit 8\n",
      "Processing 10773 samples for digit 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "handwritten_images, handwritten_labels = load_handwritten_digits()\n",
    "handwritten_images = handwritten_images.reshape(-1, 28, 28, 1)\n",
    "handwritten_labels_cat = tf.keras.utils.to_categorical(handwritten_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handwritten Digits (not MNIST) :  107,730 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"Handwritten Digits (not MNIST)\", len(handwritten_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading EMNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loading from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emnist_data():\n",
    "    train_file = os.path.join(DATASETS[\"emnist\"], \"emnist-digits-train.csv\")\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(train_file):\n",
    "            print(f\"Error: EMNIST data file not found at {train_file}\")\n",
    "            print(\"\\nAvailable files in EMNIST directory:\")\n",
    "            for file in os.listdir(DATASETS[\"emnist\"]):\n",
    "                print(f\"- {file}\")\n",
    "            raise FileNotFoundError(f\"EMNIST data file not found at {train_file}\")\n",
    "\n",
    "        print(f\"Loading EMNIST data from: {train_file}\")\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        data = pd.read_csv(train_file)\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Loaded CSV file is empty\")\n",
    "\n",
    "        labels = data.iloc[:, 0].values\n",
    "        pixels = data.iloc[:, 1:].values\n",
    "\n",
    "        images = pixels.reshape(-1, 28, 28)\n",
    "        images = images.transpose(0, 2, 1)\n",
    "        images = np.flip(images, axis=1)\n",
    "\n",
    "        images = images.astype(\"float32\") / 255.0\n",
    "\n",
    "        show_digit_3(images, labels, \"EMNIST\")\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading EMNIST data: {str(e)}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EMNIST data from: data/emnist/emnist-digits-train.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emnist_images, emnist_labels = load_emnist_data()\n",
    "emnist_images = emnist_images.reshape(-1, 28, 28, 1)\n",
    "emnist_labels_cat = tf.keras.utils.to_categorical(emnist_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMNIST                         :  239,999 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"EMNIST\", len(emnist_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading USPS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_usps_data():\n",
    "    import h5py\n",
    "\n",
    "    usps_path: str = os.path.join(DATASETS[\"usps\"], \"usps.h5\")\n",
    "\n",
    "    if not os.path.exists(usps_path):\n",
    "        raise FileNotFoundError(f\"USPS dataset file not found at {usps_path}\")\n",
    "\n",
    "    with h5py.File(usps_path, \"r\") as hf:\n",
    "        train = hf.get(\"train\")\n",
    "        X_train = train.get(\"data\")[:]\n",
    "        y_train = train.get(\"target\")[:]\n",
    "\n",
    "    print(\"Resizing USPS images from 16x16 to 28x28...\")\n",
    "    resized_images = []\n",
    "    for img in X_train:\n",
    "        img = img.reshape(16, 16)\n",
    "        img_resized = resize(img, (28, 28), anti_aliasing=True)\n",
    "        resized_images.append(img_resized)\n",
    "\n",
    "    images = np.array(resized_images)\n",
    "    images = images.astype(\"float32\")\n",
    "    if images.max() > 1.0:\n",
    "        images /= 255.0\n",
    "\n",
    "    show_digit_3(images, y_train, \"USPS\")\n",
    "\n",
    "    return images, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing USPS images from 16x16 to 28x28...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "usps_images, usps_labels = load_usps_data()\n",
    "usps_images = usps_images.reshape(-1, 28, 28, 1)\n",
    "usps_labels_cat = tf.keras.utils.to_categorical(usps_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS                           :    7,291 samples\n"
     ]
    }
   ],
   "source": [
    "print_dataset_summary(\"USPS\", len(usps_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined = np.concatenate(\n",
    "    [x_train_mnist, handwritten_images, emnist_images, usps_images]\n",
    ")\n",
    "y_train_combined = np.concatenate(\n",
    "    [y_train_mnist, handwritten_labels_cat, emnist_labels_cat, usps_labels_cat]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_combined, y_train_combined = shuffle(\n",
    "    x_train_combined, y_train_combined, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_combined, y_train_combined, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total combined samples: 415020\n",
      "Training samples: 373518\n",
      "Validation samples: 41502\n",
      "Total Combined                 :  415,020 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal combined samples: {len(x_train_combined)}\")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print_dataset_summary(\"Total Combined\", len(x_train_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define composite transform pipeline with varying probabilities\n",
    "* Combine transforms into one Compose:\n",
    "  - 100% chance (p=1.0) for ElasticTransform\n",
    "  - 20% chance (p=0.2) for GaussNoise\n",
    "  - 15% chance (p=0.15) for small 4x4 dropout\n",
    "  - 10% chance (p=0.1) for brightness/contrast\n",
    "  - 5% chance (p=0.05) to invert image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1623730/2113161089.py:6: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10, 20), p=0.2),\n",
      "/tmp/ipykernel_1623730/2113161089.py:7: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(\n"
     ]
    }
   ],
   "source": [
    "augmentation_transform = A.Compose(\n",
    "    [\n",
    "        # Geometric Transforms\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=1.0),\n",
    "        # Noise/Pixel-Level Transforms\n",
    "        A.GaussNoise(var_limit=(10, 20), p=0.2),\n",
    "        A.CoarseDropout(max_holes=1, max_height=4, max_width=4, fill_value=0.0, p=0.15),\n",
    "        # Color/Intensity Transforms\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.1),\n",
    "        A.InvertImg(p=0.05),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    # Prepare image for augmentation\n",
    "    image_0_1 = x_train[i, :, :, 0]\n",
    "    image_255 = (image_0_1 * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply augmentation pipeline\n",
    "    augmented = augmentation_transform(image=image_255)\n",
    "    aug_img_255 = augmented[\"image\"]\n",
    "\n",
    "    # Post-process augmented image\n",
    "    aug_img_01 = aug_img_255.astype(np.float32) / 255.0\n",
    "    aug_img_01 = np.expand_dims(aug_img_01, axis=-1)\n",
    "\n",
    "    augmented_images.append(aug_img_01)\n",
    "    augmented_labels.append(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combine augmented dataset with original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "augmented_images = np.array(augmented_images, dtype=np.float32)\n",
    "augmented_labels = np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and augmented datasets\n",
    "original_size = len(x_train)\n",
    "x_train = np.concatenate([x_train, augmented_images], axis=0)\n",
    "y_train = np.concatenate([y_train, augmented_labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 373518 | After augmentation: 747036\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original training size: {original_size} | After augmentation: {len(x_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras ImageDataGenerator for on-the-fly augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "steps_per_epoch = len(x_train) // 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After augmentation (per epoch):\n",
      "Training samples (now doubled): 747036\n",
      "Augmented samples per epoch: 747008\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAfter augmentation (per epoch):\")\n",
    "print(f\"Training samples (now doubled): {len(x_train)}\")\n",
    "print(f\"Augmented samples per epoch: {steps_per_epoch * 256}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting `clipnorm=1.0` to effectively \"snip\" gradients whose norm > 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 13:59:20.394800: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "clipped_adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-3,  # default LR\n",
    "    clipnorm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 convolutional blocks with increasing filters (32  64)\n",
    "* Regularization techniques:\n",
    "* BatchNormalization after each conv layer\n",
    "* SpatialDropout2D (20%) after each conv block\n",
    "* L2 regularization in dense layer\n",
    "* Dropout (50%) before final classification\n",
    "* Final 10-way softmax for classification (likely MNIST digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-17 20:27:14.333186: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        # --- Block 1 ---\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D((2, 2)),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # --- Block 2 (Modified: 96 filters) ---\n",
    "        Conv2D(96, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(96, (3, 3), activation=\"relu\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D((2, 2)),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # --- Block 3 ---\n",
    "        Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D((2, 2)),\n",
    "        SpatialDropout2D(0.2),\n",
    "        # --- Dense layers (Modified: 1050 units) ---\n",
    "        Flatten(),\n",
    "        Dense(1050, activation=\"relu\", kernel_regularizer=\"l2\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
       "\n",
       " batch_normalization              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> \n",
       "\n",
       " batch_normalization_1            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " spatial_dropout2d                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)                                                     \n",
       "\n",
       " conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">27,744</span> \n",
       "\n",
       " batch_normalization_2            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> \n",
       "\n",
       " batch_normalization_3            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " spatial_dropout2d_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)                                                     \n",
       "\n",
       " conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">110,720</span> \n",
       "\n",
       " batch_normalization_4            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> \n",
       "\n",
       " batch_normalization_5            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " spatial_dropout2d_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)                                                     \n",
       "\n",
       " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1050</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">538,650</span> \n",
       "\n",
       " batch_normalization_6            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1050</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">4,200</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                   \n",
       "\n",
       " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1050</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,510</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " conv2d (\u001b[38;5;33mConv2D\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m320\u001b[0m \n",
       "\n",
       " batch_normalization              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m128\u001b[0m \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n",
       "\n",
       " conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m9,248\u001b[0m \n",
       "\n",
       " batch_normalization_1            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m128\u001b[0m \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n",
       "\n",
       " max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " spatial_dropout2d                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mSpatialDropout2D\u001b[0m)                                                     \n",
       "\n",
       " conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m96\u001b[0m)             \u001b[38;5;34m27,744\u001b[0m \n",
       "\n",
       " batch_normalization_2            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m96\u001b[0m)                \u001b[38;5;34m384\u001b[0m \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n",
       "\n",
       " conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m96\u001b[0m)               \u001b[38;5;34m83,040\u001b[0m \n",
       "\n",
       " batch_normalization_3            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m96\u001b[0m)                  \u001b[38;5;34m384\u001b[0m \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n",
       "\n",
       " max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " spatial_dropout2d_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m96\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mSpatialDropout2D\u001b[0m)                                                     \n",
       "\n",
       " conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m110,720\u001b[0m \n",
       "\n",
       " batch_normalization_4            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m512\u001b[0m \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n",
       "\n",
       " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m147,584\u001b[0m \n",
       "\n",
       " batch_normalization_5            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m512\u001b[0m \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n",
       "\n",
       " max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " spatial_dropout2d_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mSpatialDropout2D\u001b[0m)                                                     \n",
       "\n",
       " flatten (\u001b[38;5;33mFlatten\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense (\u001b[38;5;33mDense\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1050\u001b[0m)                  \u001b[38;5;34m538,650\u001b[0m \n",
       "\n",
       " batch_normalization_6            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1050\u001b[0m)                    \u001b[38;5;34m4,200\u001b[0m \n",
       " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                   \n",
       "\n",
       " dropout (\u001b[38;5;33mDropout\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1050\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_1 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                     \u001b[38;5;34m10,510\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">934,064</span> (3.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m934,064\u001b[0m (3.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">930,940</span> (3.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m930,940\u001b[0m (3.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,124</span> (12.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,124\u001b[0m (12.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Configures the training process for the neural network with three key components:\n",
    "\n",
    "* Optimizer:\n",
    "  * Uses a custom clipped_adam optimizer\n",
    "  * Likely an Adam optimizer with gradient clipping to prevent exploding gradients\n",
    "  * Gradient clipping helps stabilize training\n",
    "\n",
    "* Loss Function:\n",
    "  * Categorical_crossentropy: Standard loss function for multi-class classification\n",
    "  * Appropriate for the 10-class MNIST digit classification task\n",
    "  * Expects one-hot encoded labels\n",
    "\n",
    "* Metrics:\n",
    "  * Tracks accuracy during training and validation\n",
    "  * Will show percentage of correctly classified images\n",
    "  * This compilation step must be done before training the model \n",
    "\n",
    "* It defines how the model will:\n",
    "  * Update weights (optimizer)\n",
    "  * Calculate errors (loss)\n",
    "  * Measure performance (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=clipped_adam,  # using the clipped optimizer\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Early Stopping\n",
    "   * Prevents overfitting by stopping training when validation loss stops improving\n",
    "   * Waits 5 epochs (patience=5) before stopping\n",
    "   * Restores the best weights found during training\n",
    "2. Model Checkpoint\n",
    "   * Saves the model when validation accuracy improves\n",
    "   * Only keeps the best performing model\n",
    "   * Saves to 'best_model.h5' in the models directory\n",
    "3. Learning Rate Reduction\n",
    "   * Reduces learning rate when validation loss plateaus\n",
    "   * Reduces by 20% (factor=0.2) after 3 epochs without improvement\n",
    "   * Won't reduce below 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(DATASETS[\"models\"], \"best_model.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "    ),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trains the model using augmented data via datagen.flow\n",
    "* Uses batch size of 256 images\n",
    "* Validates on non-augmented validation set\n",
    "* Runs for 5 epochs\n",
    "* Uses the callbacks for early stopping, checkpointing, and LR reduction\n",
    "* Stores training history in history variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 83/422\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3:24\u001b[0m 602ms/step - accuracy: 0.5108 - loss: 5.4161"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=256),\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=150,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saves the final model state after training\n",
    "* Stores it as 'final_model.h5' in the models directory\n",
    "* Different from the checkpoint callback which saves the best model during training\n",
    "* Includes model architecture, weights, and training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(DATASETS[\"models\"], \"final_model.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
