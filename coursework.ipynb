{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Training Set: 60000 samples\n",
      "MNIST Test Set: 10000 samples\n",
      "Dataset URL: https://www.kaggle.com/datasets/jcprogjava/handwritten-digits-dataset-not-in-mnist\n",
      "\n",
      "Processing Handwritten Digits Dataset (not in MNIST) from: data/handwritten-digits-not-mnist/dataset\n",
      "Processing 10773 samples for digit 0\n",
      "Processing 10773 samples for digit 1\n",
      "Processing 10773 samples for digit 2\n",
      "Processing 10773 samples for digit 3\n",
      "Processing 10773 samples for digit 4\n",
      "Processing 10773 samples for digit 5\n",
      "Processing 10773 samples for digit 6\n",
      "Processing 10773 samples for digit 7\n",
      "Processing 10773 samples for digit 8\n",
      "Processing 10773 samples for digit 9\n",
      "\n",
      "Handwritten Digits Dataset (not in MNIST) Summary:\n",
      "----------------------------------------\n",
      "\n",
      "Handwritten Digits Dataset (not in MNIST) class distribution:\n",
      "Digit 0: 10773 samples\n",
      "Digit 1: 10773 samples\n",
      "Digit 2: 10773 samples\n",
      "Digit 3: 10773 samples\n",
      "Digit 4: 10773 samples\n",
      "Digit 5: 10773 samples\n",
      "Digit 6: 10773 samples\n",
      "Digit 7: 10773 samples\n",
      "Digit 8: 10773 samples\n",
      "Digit 9: 10773 samples\n",
      "Total samples: 107730\n",
      "EMNIST Dataset: 239999 samples\n",
      "\n",
      "Total combined samples: 407729\n",
      "\n",
      "After splitting:\n",
      "Training samples: 366956\n",
      "Validation samples: 40773\n",
      "\n",
      "Original Datasets:\n",
      "----------------------------------------\n",
      "MNIST Train                    :   60,000 samples\n",
      "MNIST Test                     :   10,000 samples\n",
      "Handwritten Digits (not MNIST) :  107,730 samples\n",
      "EMNIST                         :  239,999 samples\n",
      "\n",
      "Combined Dataset:\n",
      "----------------------------------------\n",
      "Total Combined                 :  407,729 samples\n",
      "\n",
      "After Train/Val Split:\n",
      "----------------------------------------\n",
      "Training Set                   :  366,956 samples\n",
      "Validation Set                 :   40,773 samples\n",
      "\n",
      "After augmentation (per epoch):\n",
      "Original training samples: 366956\n",
      "Augmented samples per epoch: 366848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/maruf/.cache/pypoetry/virtualenvs/pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, BatchNormalization, MaxPool2D, SpatialDropout2D,\n",
    "    Flatten, Dense, Dropout\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Create consistent directory structure\n",
    "BASE_DATA_DIR = 'data'\n",
    "DATASETS: dict[str, str] = {\n",
    "    'mnist': os.path.join(BASE_DATA_DIR, 'mnist'),\n",
    "    'emnist': os.path.join(BASE_DATA_DIR, 'emnist'),\n",
    "    'handwritten_digits': os.path.join(BASE_DATA_DIR, 'handwritten-digits-not-mnist'),\n",
    "    'models': os.path.join(BASE_DATA_DIR, 'models')\n",
    "}\n",
    "\n",
    "# Create all necessary directories\n",
    "for directory in DATASETS.values():\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def print_class_distribution(labels, dataset_name):\n",
    "    \"\"\"Print distribution of classes in a dataset\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{dataset_name} class distribution:\")\n",
    "    for digit, count in zip(unique, counts):\n",
    "        print(f\"Digit {digit}: {count} samples\")\n",
    "    print(f\"Total samples: {len(labels)}\")\n",
    "\n",
    "\n",
    "def print_total_samples(labels, dataset_name):\n",
    "    \"\"\"Print only total samples in a dataset\"\"\"\n",
    "    print(f\"{dataset_name}: {len(labels)} samples\")\n",
    "\n",
    "\n",
    "def load_handwritten_digits():\n",
    "    \"\"\"Load and process Handwritten Digits Dataset (not in MNIST).\"\"\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    \n",
    "    api.dataset_download_files(\n",
    "        'jcprogjava/handwritten-digits-dataset-not-in-mnist',\n",
    "        path=DATASETS['handwritten_digits'],\n",
    "        unzip=True,\n",
    "        force=True\n",
    "    )\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    dataset_path = os.path.join(DATASETS['handwritten_digits'], 'dataset')\n",
    "    \n",
    "    print(f\"\\nProcessing Handwritten Digits Dataset (not in MNIST) from: {dataset_path}\")\n",
    "    \n",
    "    for label in range(10):\n",
    "        # Single folder_path assignment with correct path structure\n",
    "        folder_path = os.path.join(dataset_path, str(label), str(label))  # Path to digit/digit folder\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Missing directory for label {label} - {folder_path}\")\n",
    "            continue\n",
    "            \n",
    "        file_count = len([name for name in os.listdir(folder_path) if name.endswith('.png')])\n",
    "        print(f\"Processing {file_count} samples for digit {label}\")\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.png'):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert('L')\n",
    "                    img = img.resize((28, 28))\n",
    "                    img_array = np.array(img)\n",
    "                    img_array = img_array.astype('float32') / 255.0\n",
    "                    img_array = 1.0 - img_array  # Invert to white-on-black\n",
    "                    images.append(img_array)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(\"\\nHandwritten Digits Dataset (not in MNIST) Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_class_distribution(labels, \"Handwritten Digits Dataset (not in MNIST)\")\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_emnist_data():\n",
    "    \"\"\"Load and process EMNIST digits dataset.\"\"\"\n",
    "    # Download and load EMNIST data\n",
    "    emnist_train_df = kagglehub.load_dataset(\n",
    "        KaggleDatasetAdapter.PANDAS,\n",
    "        \"crawford/emnist\",\n",
    "        \"emnist-digits-train.csv\"\n",
    "    )\n",
    "    \n",
    "    # Process EMNIST data\n",
    "    labels = emnist_train_df.iloc[:, 0].values\n",
    "    pixels = emnist_train_df.iloc[:, 1:].values\n",
    "    \n",
    "    # Reshape and reorient images\n",
    "    images = pixels.reshape(-1, 28, 28)\n",
    "    images = images.transpose(0, 2, 1)  # Correct orientation\n",
    "    images = np.flip(images, axis=1)  # Vertical flip\n",
    "    \n",
    "    # Normalize\n",
    "    images = images.astype('float32') / 255.0\n",
    "    \n",
    "    print_total_samples(labels, \"EMNIST Dataset\")\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_mnist_data():\n",
    "    \"\"\"Load and process MNIST dataset.\"\"\"\n",
    "    (x_train, labels_train), (x_test, labels_test) = mnist.load_data()\n",
    "    print_total_samples(labels_train, \"MNIST Training Set\")\n",
    "    print_total_samples(labels_test, \"MNIST Test Set\")\n",
    "    \n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    return x_train, labels_train, x_test, labels_test\n",
    "\n",
    "\n",
    "def print_dataset_summary(dataset_name, count):\n",
    "    print(f\"{dataset_name:<30} : {count:>8,d} samples\")\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Prepare and combine all datasets.\"\"\"\n",
    "    # Load MNIST\n",
    "    x_train_mnist, labels_train_mnist, x_test, labels_test = load_mnist_data()\n",
    "    x_train_mnist = x_train_mnist.reshape(-1, 28, 28, 1)\n",
    "    y_train_mnist = tf.keras.utils.to_categorical(labels_train_mnist, 10)\n",
    "    \n",
    "    # Load handwritten digits data\n",
    "    handwritten_images, handwritten_labels = load_handwritten_digits()\n",
    "    handwritten_images = handwritten_images.reshape(-1, 28, 28, 1)\n",
    "    handwritten_labels_cat = tf.keras.utils.to_categorical(handwritten_labels, 10)\n",
    "    \n",
    "    # Load EMNIST\n",
    "    emnist_images, emnist_labels = load_emnist_data()\n",
    "    emnist_images = emnist_images.reshape(-1, 28, 28, 1)\n",
    "    emnist_labels_cat = tf.keras.utils.to_categorical(emnist_labels, 10)\n",
    "    \n",
    "    # Combine datasets\n",
    "    x_train_combined = np.concatenate([x_train_mnist, handwritten_images, emnist_images])\n",
    "    y_train_combined = np.concatenate([y_train_mnist, handwritten_labels_cat, emnist_labels_cat])\n",
    "    \n",
    "    print(f\"\\nTotal combined samples: {len(x_train_combined)}\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    x_train_combined, y_train_combined = shuffle(x_train_combined, y_train_combined, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train_combined, y_train_combined, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAfter splitting:\")\n",
    "    print(f\"Training samples: {len(x_train)}\")\n",
    "    print(f\"Validation samples: {len(x_val)}\")\n",
    "    \n",
    "    print(\"\\nOriginal Datasets:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"MNIST Train\", len(labels_train_mnist))\n",
    "    print_dataset_summary(\"MNIST Test\", len(labels_test))\n",
    "    print_dataset_summary(\"Handwritten Digits (not MNIST)\", len(handwritten_labels))\n",
    "    print_dataset_summary(\"EMNIST\", len(emnist_labels))\n",
    "    \n",
    "    # After combining\n",
    "    total_samples = len(x_train_combined)\n",
    "    print(\"\\nCombined Dataset:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"Total Combined\", total_samples)\n",
    "    \n",
    "    # After splitting\n",
    "    print(\"\\nAfter Train/Val Split:\")\n",
    "    print(\"-\" * 40)\n",
    "    print_dataset_summary(\"Training Set\", len(x_train))\n",
    "    print_dataset_summary(\"Validation Set\", len(x_val))\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val\n",
    "\n",
    "# Prepare data\n",
    "x_train, y_train, x_val, y_val = prepare_data()\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "steps_per_epoch = len(x_train) // 256  # batch_size = 256\n",
    "print(f\"\\nAfter augmentation (per epoch):\")\n",
    "print(f\"Original training samples: {len(x_train)}\")\n",
    "print(f\"Augmented samples per epoch: {steps_per_epoch * 256}\")\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D((2,2)),\n",
    "    SpatialDropout2D(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Training callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(DATASETS['models'], 'best_model.h5'),\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=256),\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Save final model in HDF5 format\n",
    "model.save(os.path.join(DATASETS['models'], 'final_model.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-recognition-neural-network-coursew-8OHAYx0u-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
